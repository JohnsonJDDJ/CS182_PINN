{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Networks\n",
    "\n",
    "Original [paper](https://www.sciencedirect.com/science/article/pii/S0021999118307125) by M.Raissi et al. \n",
    "\n",
    "In complex physical, biological or engineering systems, obtaining data is sometimes unachievable. State of the art machine learning techniques cannot provide any guarantee of convergence given the lack of training data. \n",
    "\n",
    "**Traditional physics model** creation is a task of a domain expert, who parametrises physics models to best fit a system of interest. For example, creating a model of aircraft dynamics using equations of drag, lift, gravity, thrust, etc., and parametrising the model to attempt to closely match the model to a specific aircraft.\n",
    "\n",
    "However, sometimes physical systems are hard to parametrise and there does not exist a closed form solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 8})\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import value_and_grad, jit, random, vmap\n",
    "import optax\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "KEY = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solution to the Shrodinger Equation\n",
    "\n",
    "As an example to illustrate how PINNs are applied, we will look at the solution to the non-linear Shrodinger equation.\n",
    "\n",
    "The non-linear Shrodinger equation, along with its periodic boundary conditions is given by: $$i\\frac{\\partial h(x, t)}{\\partial t} + 0.5 \\frac{\\partial^2 h(x, t)}{\\partial x \\partial x} + |h(x, t)|^2h(x, t) = 0$$ $$h(x, 0) = 2 sech(x)$$ $$h(-5, t) = h(5, t)$$ $$\\frac{\\partial h(-5, t)}{\\partial x} = \\frac{\\partial h(5, t)}{\\partial x}$$\n",
    "\n",
    "The complex valued solution of the non-linear Schrodinger equaton is denoted as $$h(x, t), x\\in[-5, 5], t \\in [0, \\pi/2]$$ The output of $h(x, t)$ will be $u + iv$, where $u$ is the real part and $v$ is the imaginary part. To better understand the solution and for easier visualization, we will inspect the norm of our solution $|h(x,t)| = \\sqrt{u^2 + v^2}$ for the rest of the homework.\n",
    "\n",
    "Below, we will import our dataset associated with the Shrodinger equation. The data include input values $x, t$ and it's corresponding output values $u, v$ pair. Please just run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat(\"original_paper/Data/NLS.mat\")\n",
    "\n",
    "# Global variables\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "N0 = 50\n",
    "N_b = 50\n",
    "N_f = 20000\n",
    "\n",
    "# Original data (including training and validation)\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "u = np.real(data['uu'])\n",
    "v = np.imag(data['uu'])\n",
    "h = np.sqrt(u**2 + v**2)\n",
    "\n",
    "# Initial Condition Data (N = 50)\n",
    "idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "x0 = x[idx_x,:]\n",
    "t0 = np.zeros(x0.shape)\n",
    "u0 = u[idx_x,0:1]\n",
    "v0 = v[idx_x,0:1]\n",
    "X0 = np.concatenate([x0, t0], axis=1)\n",
    "Y0 = np.concatenate([u0, v0], axis=1)\n",
    "\n",
    "# Boundary Condition Data (N = 50 + 50)\n",
    "idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "tb = t[idx_t,:]\n",
    "t_ub = t_lb = tb\n",
    "X_ub = np.concatenate([np.zeros(tb.shape) + ub[0], t_ub], axis=1)\n",
    "X_lb = np.concatenate([np.zeros(tb.shape) + lb[0], t_lb], axis=1)\n",
    "Xb = np.concatenate([X_ub, X_lb])\n",
    "u_lb = u[0, idx_t][..., None]\n",
    "u_ub = u_lb\n",
    "v_lb = v[0, idx_t][..., None]\n",
    "v_ub = v_lb\n",
    "Y_ub = np.concatenate([u_ub, v_ub], axis=1)\n",
    "Y_lb = np.concatenate([u_lb, v_lb], axis=1)\n",
    "Yb = np.concatenate([Y_ub, Y_lb])\n",
    "\n",
    "# Training data (N = 50 + (50 + 50))\n",
    "X = np.concatenate([X0, Xb])\n",
    "Y = np.concatenate([Y0, Yb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial condition training data. X0: {X0.shape}, Y0: {Y0.shape}\")\n",
    "print(f\"Boundary condition training data. Xb: {Xb.shape}, Yb: {Yb.shape}\")\n",
    "print(f\"Total training data. X: {X.shape}, Y: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we only have 150 training data. In reality, with reasonable effort, we could only obtain the data at the initial and boundary conditions of the Shrodinger equation. The initial condition is when $t=0$, and the boundary condition is when $x=-5$ or $x=5$. \n",
    "\n",
    "In the paper, we are limited with 50 data points at the initial condition, 50 data points at the upper boundary condition, and 50 data points at the lower boundary condition. Combined, we have 150 data points to train our neural network. This is not desired, and we will see how vanilla MLP will perform in a low-data regime.\n",
    "\n",
    "\n",
    "### Visualizing our full data\n",
    "\n",
    "Before training our neural network, let us visualize how our full data look like. The full data include data that is not limited to our initial and boundary conditions. As mentioned above, we will visualize the complex valued solution through $|h(x, t)|$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_grid, x_grid = np.meshgrid(t,x)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.view_init(20, 35)\n",
    "ax.scatter(x_grid, t_grid, h, c=h, cmap='viridis')\n",
    "ax.set(xlabel=\"x\", ylabel=\"t\", title=\"Full Schrodinger Data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our training data on initial condition (t=0)\n",
    "\n",
    "Now let us visualize the solution of our training data at the initial condition. We will visualize the real and imaginary part of our solution seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 3))\n",
    "axs[0].scatter(X0[:, 0], Y0[:, 0])\n",
    "axs[1].scatter(X0[:, 0], Y0[:, 1])\n",
    "axs[0].set(xlabel=\"x\", ylabel=\"u: real part of h(x, t)\", title=\"Real part at initial condition when t=0\")\n",
    "axs[1].set(xlabel=\"x\", ylabel=\"v: imaginary part of h(x, t)\", title=\"Imaginary part at initial condition when t=0\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our training data on boundary condition (x=-5 and x=5)\n",
    "\n",
    "Now let us visualize the solution at the boundary conditions. Note that the solution for $h(-5, t)$ and $h(5, t)$ for all $t \\in [0, \\pi/2]$ is exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 3))\n",
    "axs[0].scatter(Xb[:50, 1], Yb[:50, 0])\n",
    "axs[1].scatter(Xb[:50, 1], Yb[:50, 1])\n",
    "axs[0].set(xlabel=\"t\", ylabel=\"u: real part of h(x, t)\", title=\"Real part at initial condition when x=-5 and x=5\")\n",
    "axs[1].set(xlabel=\"t\", ylabel=\"v: imaginary part of h(x, t)\", title=\"Imaginary part at initial condition when x=-5 and x=5\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla MLP\n",
    "\n",
    "Ok! We are ready to find the solution of the Shrodinger equation using traditional neural network architectures.\n",
    "\n",
    "In traditional data-driven methods, we are trying to find the solution h(x,t) using a neural network and our training data described above. Here, according to the paper, the architecture is a traditional 5-layer MLP with $\\tanh$ as activation functions. Specifically: $$NN(X) = W^{(5)}(\\tanh(W^{(4)}\\tanh(\\dots) + b^{(4)})) + b^{(5)}$$ where $X = [x, t]$ is our input vector of shape (2), and $W^{(n)}, b^{(n)}$ indicate the weights and biases at each layer. \n",
    "\n",
    "According to the paper, the dimension for each layer is: $[2, 100, 100, 100, 100, 2]$. The output is of shape (2) in the form of $[u, v]$, the real and imaginary part of $h(x, t)$.\n",
    "\n",
    "First implement `init_params` and `vanilla_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layers, key=KEY):\n",
    "    '''\n",
    "    Initialize parameters in the MLP. Weights are initialized\n",
    "    using Xavier initialization, while biases are zero initialized.\n",
    "\n",
    "    Arguments\n",
    "    - layers: A list indicating the dimension of each individual\n",
    "      imbedding at layers of MLP. For example: [1, 10, 1] would\n",
    "      indicate a simple 2 layer MLP. \n",
    "\n",
    "    Returns\n",
    "    - params: the initialized parameters. For example, \n",
    "      [(w_1, b_1), (w_2, b_2)] is a valid output.\n",
    "    '''\n",
    "    def xavier_init(input_dim, output_dim, key=key):\n",
    "        '''Use Xavier initialization for weights of a single layer'''\n",
    "        w = None\n",
    "        # ===== START OF YOUR CODE =====\n",
    "        # compute standard deviation for xavier init\n",
    "        # recall it is given by sqrt{2 /(?)}\n",
    "        std_dev = # TODO: compute standard deviation\n",
    "        w =  # TODO: initialize the weights\n",
    "        # ===== END OF YOUR CODE =====\n",
    "        return w\n",
    "\n",
    "    params = []\n",
    "\n",
    "    # ===== START OF YOUR CODE =====\n",
    "    for l in : # TODO\n",
    "        w = # TODO: xavier initialize the weight\n",
    "        b = # TODO: zero initialize the bias\n",
    "        params.append() # TODO: append weight and bias for this layer to params\n",
    "    # ===== END OF YOUR CODE =====\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def vanilla_forward(params, x):\n",
    "    '''\n",
    "    Forward pass through the vanilla MLP. In PINN, the nonlinearity are\n",
    "    applied using tanh.\n",
    "\n",
    "    Arguments\n",
    "    - params: weights and biases for all layers of the vanilla MLP.\n",
    "      For example, [(w_1, b_1), (w_2, b_2)] is a valid input.\n",
    "    - x: input to the vanilla MLP\n",
    "\n",
    "    Returns\n",
    "    - out: output of the vanilla MLP\n",
    "    '''\n",
    "    activations = x\n",
    "\n",
    "    # ===== START OF YOUR CODE =====\n",
    "    # Create a for loop to loop over the parameters EXCEPT\n",
    "    # the parameters corresponding to the very last layer.\n",
    "    # This is because we will not be applying the nonlinearity\n",
    "    # to the output of the last layer (which is out prediction).\n",
    "    for w, b in :# TODO\n",
    "        out = # TODO: Perform linear operation\n",
    "        activations = # TODO: apply tanh activation\n",
    "    \n",
    "    final_w, final_b = # TODO\n",
    "    out = # TODO: Do not apply nonlinearity to last layer\n",
    "    # ===== END OF YOUR CODE =====\n",
    "    return out\n",
    "\n",
    "# Use vmap to perform in parallel (jax magic)\n",
    "vanilla_batched_forward = vmap(vanilla_forward, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test implementation for `init_param` and `vanilla_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2, 10, 1]\n",
    "x = random.uniform(KEY, (5, 2))\n",
    "out = vanilla_batched_forward(init_params(layers), x)\n",
    "expected = jnp.array([[-0.05742961],\n",
    "                      [-0.08960884],\n",
    "                      [-0.04750253],\n",
    "                      [-0.17843515],\n",
    "                      [-0.09102767]])\n",
    "assert jnp.allclose(out, expected)\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement `vanilla_mseloss` and `step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_mseloss(params, x, y):\n",
    "    '''\n",
    "    Compute mean squared error between x and y\n",
    "\n",
    "    Arguments\n",
    "    - layers: all params for the MLP\n",
    "    - x: input to the MLP\n",
    "    - y: true label\n",
    "    '''\n",
    "    loss = None\n",
    "    # ===== START OF YOUR CODE =====\n",
    "    pred =  # TODO: use `vanilla_batched_forward` to make prediction\n",
    "    loss =  # TODO\n",
    "    # ===== END OF YOUR CODE =====\n",
    "    return loss\n",
    "\n",
    "def vanilla_fit(params, optimizer, X, Y):\n",
    "    '''\n",
    "    Train the vanilla MLP using training data X and Y with a given optimizer.\n",
    "    \n",
    "    Arguments:\n",
    "    - params: weights and biases for all layers of the MLP.\n",
    "    - optimizer: An optax optimizer.\n",
    "    - X: Input to the model.\n",
    "    - Y: Labels.\n",
    "    \n",
    "    Returns:\n",
    "    Trained parameters.\n",
    "    '''\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    @jit\n",
    "    def step(params, opt_state, x, y):\n",
    "        '''\n",
    "        A single step of our training algorithm.\n",
    "        \n",
    "        Arguments:\n",
    "        - params: weights and biases for all layers.\n",
    "        - opt_state: Optimizer state.\n",
    "        - x: One input.\n",
    "        - y: One label\n",
    "        \n",
    "        Returns:\n",
    "        - params: updated parameters\n",
    "        - opt_State: updated optimizer state\n",
    "        - loss_value: loss of this run\n",
    "        '''\n",
    "        # ===== START OF YOUR CODE =====\n",
    "        loss_value, grads = # TODO: check out `value_and_grad` using jax doc\n",
    "        updates, opt_state = # TODO: check out `optimizer.update()` using jax doc\n",
    "        params = # TODO: check out `apply_updates()` using jax doc\n",
    "        # ===== END OF YOUR CODE =====\n",
    "        return params, opt_state, loss_value\n",
    "\n",
    "    for epoch in tqdm(range(1000)):\n",
    "        for (x, y) in zip(X, Y):\n",
    "            # ===== START OF YOUR CODE =====\n",
    "            params, opt_state, loss_value = # TODO: Execute one step of fitting\n",
    "            # ===== END OF YOUR CODE =====\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to train the vanilla MLP! We will train it with 1000 epochs. It will take approx 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2, 100, 100, 100, 100, 2]\n",
    "optimizer = optax.adam(learning_rate=5e-4)\n",
    "\n",
    "# ===== START OF YOUR CODE =====\n",
    "vanilla_trained_params = # TODO: fit your vanilla MLP\n",
    "# ===== END OF YOUR CODE ====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize your training result\n",
    "\n",
    "First visualize the performance on the training data. Note: It will look quite messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== START OF YOUR CODE =====\n",
    "pred = # TODO: make prediction using `vanilla_trained_params`\n",
    "# ===== END OF YOUR CODE =====\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(12, 8))\n",
    "axs[0,0].scatter(X[:50, 0], pred[:50, 0], label=\"prediction\")\n",
    "axs[0,0].scatter(X[:50, 0], Y0[:, 0], label=\"true value\")\n",
    "axs[0,0].legend()\n",
    "axs[0,1].scatter(X[:50, 0], pred[:50, 1], label=\"prediction\")\n",
    "axs[0,1].scatter(X[:50, 0], Y0[:, 1], label=\"true value\")\n",
    "axs[0,1].legend()\n",
    "axs[0,0].set(xlabel=\"x\", ylabel=\"u: real part of h(x, t)\", title=\"Real part at initial condition when t=0\")\n",
    "axs[0,1].set(xlabel=\"x\", ylabel=\"v: imaginary part of h(x, t)\", title=\"Imaginary part at initial condition when t=0\")\n",
    "axs[1,0].scatter(X[50:100, 1], pred[50:100, 0], label=\"lower bound prediction\")\n",
    "axs[1,0].scatter(X[100:, 1], pred[100:, 0], label=\"upper bound prediction\")\n",
    "axs[1,0].scatter(X[50:100, 1], Yb[0:50, 0], label=\"true value\")\n",
    "axs[1,0].legend()\n",
    "axs[1,1].scatter(X[50:100, 1], pred[50:100, 1], label=\"lower bound prediction\")\n",
    "axs[1,1].scatter(X[100:, 1], pred[100:, 1], label=\"upper bound prediction\")\n",
    "axs[1,1].scatter(X[50:100, 1], Yb[0:50, 1], label=\"true value\")\n",
    "axs[1,1].legend()\n",
    "axs[1,0].set(xlabel=\"t\", ylabel=\"u: real part of h(x, t)\", title=\"Real part at boundary condition when x=-5 and x=5\")\n",
    "axs[1,1].set(xlabel=\"t\", ylabel=\"v: imaginary part of h(x, t)\", title=\"Imaginary part at boundary condition when x=-5 and x=5\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize the performance on the full data. Note: It will look quite messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_all = t_grid.flatten()[..., None]\n",
    "x_all = x_grid.flatten()[..., None]\n",
    "X_all = np.concatenate([x_all, t_all], axis=1)\n",
    "\n",
    "pred = vanilla_batched_forward(vanilla_trained_params, X_all)\n",
    "h_pred_vanilla = np.sqrt(pred[:,0]**2 + pred[:,1]**2)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.view_init(20, 35)\n",
    "ax.scatter(x_all, t_all, h_pred_vanilla, c=h_pred_vanilla, cmap='viridis')\n",
    "ax.set(xlabel=\"x\", ylabel=\"t\", title=\"Predicted Schrodinger data using vanilla MLP\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_prediction_error = jnp.mean(jnp.square(h_pred_vanilla - h.flatten()))\n",
    "print(f\"The total validation error for the vanilla MLP is: {vanilla_prediction_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN\n",
    "\n",
    "You might notice that we haven't really used any knowledge about the Schrodinger equation when training the vanilla MLP. We are simply feeding out training data, treating the MLP as a black box and trust its power to approximate the Schrodinger's solution.\n",
    "\n",
    "But since we only have 150 training data points, our vanilla MLP cannot reach an optimized result. Thus, we now look at how to incorporate our knowledge on the Shrodinger's equation in to our vanilla MLP architecture.\n",
    "\n",
    "To begin, we need to slightly modify the `forward` function to follow some conventions of `JAX`.\n",
    "\n",
    "1. We want our `forward` function to return scalar values so the `JAX` library can take the gradients easily. Since currently, our implementation of `forward` returns $[u, v]$, we need to split our forward function into two.\n",
    "2. We also want to pass our inputs into the `forward` function seperatly. Previously, we are feeding in $[x, t]$. Now, we need to feed them in seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinn_forward_u(params, x, t):\n",
    "    '''\n",
    "    Forward pass through the PINN that outputs u(the real part of the result).\n",
    "    Notice that we create two forward functions for u(real part) and v(imaginary part) seperately\n",
    "    because in order for JAX to do auto-grad, the function must output a scalar value, not a tuple.\n",
    "    \n",
    "    The nonlinearity used here is tanh.\n",
    "    \n",
    "    Arguments\n",
    "    - params: weights and biases for all layers of the vanilla MLP.\n",
    "      For example, [(w_1, b_1), (w_2, b_2)] is a valid input.\n",
    "    - x: input to PINN\n",
    "    - t: input to PINN\n",
    "\n",
    "    Returns\n",
    "    - out: real part of the output of PINN\n",
    "    '''\n",
    "    activations = jnp.hstack([x, t])\n",
    "    \n",
    "    # ===== START OF YOUR CODE =====\n",
    "    for w, b in : # TODO\n",
    "        out =  # TODO: Perform linear operation\n",
    "        activations =  # TODO: apply tanh activation\n",
    "    \n",
    "    final_w, final_b = # TODO\n",
    "    out = # TODO: Do not apply nonlinearity to last layer\n",
    "    # ===== END OF YOUR CODE =====\n",
    "    return out[0]\n",
    "  \n",
    "def pinn_forward_v(params, x, t):\n",
    "    '''\n",
    "    Forward pass through the PINN that outputs v(the imaginary part of the result).\n",
    "    Notice that we create two forward functions for u(real part) and v(imaginary part) seperately\n",
    "    because in order for JAX to do auto-grad, the function must output a scalar value, not a tuple.\n",
    "    \n",
    "    The nonlinearity used here is tanh.\n",
    "\n",
    "    Arguments\n",
    "    - params: weights and biases for all layers of the vanilla MLP.\n",
    "      For example, [(w_1, b_1), (w_2, b_2)] is a valid input.\n",
    "    - x: input to PINN\n",
    "    - t: input to PINN\n",
    "\n",
    "    Returns\n",
    "    - out: imaginary part of the output of PINN\n",
    "    '''\n",
    "    activations = jnp.hstack([x, t])\n",
    "    \n",
    "    # ===== START OF YOUR CODE =====\n",
    "    for w, b in : # TODO\n",
    "        out = # TODO: Perform linear operation\n",
    "        activations =  # TODO: apply tanh activation\n",
    "    \n",
    "    final_w, final_b = # TODO\n",
    "    out = # TODO: Do not apply nonlinearity to last layer\n",
    "    # ===== END OF YOUR CODE =====\n",
    "    return out[1]\n",
    "\n",
    "pinn_batched_forward_u = vmap(pinn_forward_u, in_axes=(None, 0, 0))\n",
    "pinn_batched_forward_v = vmap(pinn_forward_v, in_axes=(None, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinn forward test\n",
    "result = pinn_batched_forward_u(vanilla_trained_params, X[:,0], X[:,1])\n",
    "expected = vanilla_batched_forward(vanilla_trained_params, X)[:, 0]\n",
    "assert jnp.allclose(result, expected)\n",
    "\n",
    "result = pinn_batched_forward_v(vanilla_trained_params, X[:,0], X[:,1])\n",
    "expected = vanilla_batched_forward(vanilla_trained_params, X)[:, 1]\n",
    "assert jnp.allclose(result, expected)\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the most important part of the homework. The essence of incorporating the Schrodinger's equation into the architecture is through modifying the loss functions. Recall in the Schrodinger's data, we can classify our data into three categories: the initial conditions, the boundary conditions, and the \"normal\" data. Let's look at them individually.\n",
    "\n",
    "### MSE Loss designed for initial condition $MSE_0$\n",
    "\n",
    "This is the simplest case. Since we have labeled training data, we will just use the traditional MSE loss. Nothing changed.\n",
    "\n",
    "### MSE Loss designed for boundary condition $MSE_b$\n",
    "\n",
    "In this case, thing got a bit more interesting. We also have labeled training data. But on top of that, the Shrodinger's equation states that $$h(-5, t) = h(5, t)$$ $$\\frac{\\partial h(-5, t)}{\\partial x} = \\frac{\\partial h(5, t)}{\\partial x}$$ In other words, the solution and the derivative of our solution with respect to $x$ should be symmetrical! We can incorporate this knowlegde by designing optimizer to minimize $MSE_b$ defined as: $$MSE_b := \\frac{1}{N}\\sum^N_{i=0}\\left[\\|h(-5, t_i) - h(5, t_i)\\|^2 + \\|\\frac{\\partial}{\\partial x}h(-5, t_i) - \\frac{\\partial}{\\partial x}h(5, t_i)\\|^2\\right]$$\n",
    "\n",
    "### MSE Loss designed for \"normal\" $MSE_f$\n",
    "\n",
    "The most intereting case. In \"normal\" case, we do not have labeled data. However, it turns out we can design a loss function that will motivate our neural network to obey the Shrodinger's equation where we do have labeled data. Specifically, $$i\\frac{\\partial h(x, t)}{\\partial t} + 0.5 \\frac{\\partial^2 h(x, t)}{\\partial x \\partial x} + |h(x, t)|^2h(x, t) = 0$$ By looking at the equation, we can see that the desired outcome for the LHS is zero. Therefore, if we define the LHS of the equation as $$f(x, h):= i\\frac{\\partial h(x, t)}{\\partial t} + 0.5 \\frac{\\partial^2 h(x, t)}{\\partial x \\partial x} + |h(x, t)|^2h(x, t)$$, we can design the following loss function to minimize: $$MSE_f := \\frac{1}{N}\\sum^N_{i=0}\\|f(x_i, t_i)\\|^2$$\n",
    "\n",
    "During traning, the optimizer will aim to make $f(x,h) = 0$ to minimize $MSE_f$, which is exactly what we desire.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "$$MSE = MSE_0 + MSE_b + MSE_f$$ \n",
    "\n",
    "$$MSE_0 := \\frac{1}{N}\\sum^N_{i=0}\\|h(x_i, t_i) - h_i\\|$$\n",
    "\n",
    "$$MSE_b := \\frac{1}{N}\\sum^N_{i=0}\\left[\\|h(-5, t_i) - h(5, t_i)\\|^2 + \\|\\frac{\\partial}{\\partial x}h(-5, t_i) - \\frac{\\partial}{\\partial x}h(5, t_i)\\|^2\\right]$$\n",
    "\n",
    "$$MSE_f := \\frac{1}{N}\\sum^N_{i=0}\\|f(x_i, t_i)\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinn_mseloss(params, x, t, u_true, v_true):\n",
    "    '''\n",
    "    Compute the MSE Loss of the PINN. Following the Schrodinger's\n",
    "    equation, the MSE Loss is a combination of 3 mse losses:\n",
    "    mse_zero, mse_boundary, and mse_f.\n",
    "    \n",
    "    Arguments:\n",
    "    - params: weights and biases for all layers of the PINN.\n",
    "    - x, t, u_true, v_true: input to the PINN\n",
    "    \n",
    "    Returns: A combined MSE Loss.\n",
    "    '''\n",
    "    \n",
    "    # ===== START OF YOUR CODE =====\n",
    "    u = # TODO: do a forward pass\n",
    "    v = # TODO: do a forward pass\n",
    "\n",
    "    # TODO: Compute mseloss_0\n",
    "    mseloss_zero = \n",
    "    \n",
    "    # TODO: Compute mseloss_boundary\n",
    "    x_opposite = jnp.abs(x)\n",
    "    u_opposite = \n",
    "    v_opposite = \n",
    "    # TODO: Compute du/dx and du/dt; Hint: use jacobian reversed\n",
    "    u_x, u_t =  \n",
    "    v_x, v_t = \n",
    "    u_x_opposite = \n",
    "    v_x_opposite = \n",
    "\n",
    "    mseloss_boundary = \n",
    "\n",
    "    # TODO: Compute mseloee_f\n",
    "    # Hint: Follow the equation above\n",
    "    u_xx = \n",
    "    v_xx = \n",
    "    f_u = \n",
    "    f_v = \n",
    "\n",
    "    mseloss_f = \n",
    "\n",
    "    loss = # TODO: Combine all 3 losses to get the final loss\n",
    "    # ===== END OF YOUR CODE =====\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def pinn_fit(params, optimizer, X, Y):\n",
    "    '''\n",
    "    Train PINN using training data X and Y with a given optimizer.\n",
    "    \n",
    "    Arguments:\n",
    "    - params: weights and biases for all layers of the PINN.\n",
    "    - optimizer: An optax optimizer.\n",
    "    - X: Input of the PINN\n",
    "    - Y: Labels\n",
    "    \n",
    "    Returns:\n",
    "    Trained parameters.\n",
    "    '''\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    @jit\n",
    "    def step(params, opt_state, x, t, u_true, v_true):\n",
    "        '''\n",
    "        A single step of our training algorithm.\n",
    "        \n",
    "        Arguments:\n",
    "        - params: weights and biases for all layers.\n",
    "        - opt_state: Optimizer state.\n",
    "        - x: One input.\n",
    "        - y: One label\n",
    "        \n",
    "        Returns:\n",
    "        - params: updated parameters\n",
    "        - opt_State: updated optimizer state\n",
    "        - loss_value: loss of this run\n",
    "        '''\n",
    "        # ===== START OF YOUR CODE =====\n",
    "        loss_value, grads = # TODO: check out `value_and_grad` using jax doc\n",
    "        updates, opt_state =  # TODO: check out `optimizer.update()` using jax doc\n",
    "        params =  # TODO: check out `apply_updates()` using jax doc\n",
    "        # ===== END OF YOUR CODE =====\n",
    "        return params, opt_state, loss_value\n",
    "\n",
    "    for epoch in tqdm(range(1000)):\n",
    "        for (x, y) in zip(X, Y):\n",
    "            # ===== START OF YOUR CODE =====\n",
    "            # TODO: split our input and output\n",
    "            x, t = \n",
    "            u_true, v_true = \n",
    "            params, opt_state, loss_value = # TODO: run one step\n",
    "            # ===== END OF YOUR CODE =====\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our PINN! This will take approximately 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2, 100, 100, 100, 100, 2]\n",
    "optimizer = optax.adam(learning_rate=5e-4)\n",
    "\n",
    "# ===== START OF YOUR CODE =====\n",
    "pinn_trained_params = # TODO: train the PINN\n",
    "# ===== END OF YOUR CODE ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== START OF YOUR CODE =====\n",
    "pred = # TODO: make prediction using `trained_params`\n",
    "# ===== END OF YOUR CODE =====\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(12, 8))\n",
    "axs[0,0].scatter(X[:50, 0], pred[:50, 0], label=\"prediction\")\n",
    "axs[0,0].scatter(X[:50, 0], Y0[:, 0], label=\"true value\")\n",
    "axs[0,0].legend()\n",
    "axs[0,1].scatter(X[:50, 0], pred[:50, 1], label=\"prediction\")\n",
    "axs[0,1].scatter(X[:50, 0], Y0[:, 1], label=\"true value\")\n",
    "axs[0,1].legend()\n",
    "axs[0,0].set(xlabel=\"x\", ylabel=\"u: real part of h(x, t)\", title=\"Real part at initial condition when t=0\")\n",
    "axs[0,1].set(xlabel=\"x\", ylabel=\"v: imaginary part of h(x, t)\", title=\"Imaginary part at initial condition when t=0\")\n",
    "axs[1,0].scatter(X[50:100, 1], pred[50:100, 0], label=\"lower bound prediction\")\n",
    "axs[1,0].scatter(X[100:, 1], pred[100:, 0], label=\"upper bound prediction\")\n",
    "axs[1,0].scatter(X[50:100, 1], Yb[0:50, 0], label=\"true value\")\n",
    "axs[1,0].legend()\n",
    "axs[1,1].scatter(X[50:100, 1], pred[50:100, 1], label=\"lower bound prediction\")\n",
    "axs[1,1].scatter(X[100:, 1], pred[100:, 1], label=\"upper bound prediction\")\n",
    "axs[1,1].scatter(X[50:100, 1], Yb[0:50, 1], label=\"true value\")\n",
    "axs[1,1].legend()\n",
    "axs[1,0].set(xlabel=\"t\", ylabel=\"u: real part of h(x, t)\", title=\"Real part at boundary condition when x=-5 and x=5\")\n",
    "axs[1,1].set(xlabel=\"t\", ylabel=\"v: imaginary part of h(x, t)\", title=\"Imaginary part at boundary condition when x=-5 and x=5\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_all = t_grid.flatten()[..., None]\n",
    "x_all = x_grid.flatten()[..., None]\n",
    "X_all = np.concatenate([x_all, t_all], axis=1)\n",
    "\n",
    "pred = vanilla_batched_forward(pinn_trained_params, X_all)\n",
    "h_pred_pinn = np.sqrt(pred[:,0]**2 + pred[:,1]**2)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.view_init(20, 35)\n",
    "ax.scatter(x_all, t_all, h_pred_pinn, c=h_pred_pinn, cmap='viridis')\n",
    "ax.set(xlabel=\"x\", ylabel=\"t\", title=\"Predicted Schrodinger data using PINN\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_prediction_error = jnp.mean(jnp.square(h_pred_pinn - h.flatten()))\n",
    "print(f\"The total validation error for the PINN is: {pinn_prediction_error}\")\n",
    "print(f\"The total validation error for the vanilla MLP is: {vanilla_prediction_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "M. Raissi, P. Perdikaris, G.E. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations, Journal of Computational Physics, 2019. https://www.sciencedirect.com/science/article/pii/S0021999118307125"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "d750338c8a943fc481a1620ad51f5c8f8396d3ac2c25ef6013f469b482fa22a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
