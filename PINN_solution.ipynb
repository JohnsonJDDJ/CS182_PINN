{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Networks\n",
    "\n",
    "Original [paper](https://www.sciencedirect.com/science/article/pii/S0021999118307125) by M.Raissi et al. \n",
    "\n",
    "In complex physical, biological or engineering systems, obtaining data is sometimes unachievable. State of the art machine learning techniques cannot provide any guarantee of convergence given the lack of training data. \n",
    "\n",
    "**Traditional physics model** creation is a task of a domain expert, who parametrises physics models to best fit a system of interest. For example, creating a model of aircraft dynamics using equations of drag, lift, gravity, thrust, etc., and parametrising the model to attempt to closely match the model to a specific aircraft.\n",
    "\n",
    "However, sometimes physical systems are hard to parametrise and there does not exist a closed form solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import value_and_grad, jit\n",
    "from jax import random, vmap\n",
    "import optax\n",
    "\n",
    "KEY = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional MLP\n",
    "\n",
    "First implement `init_params` and `forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layers, key=KEY):\n",
    "    '''\n",
    "    Initialize parameters in the MLP. Weights are initialized\n",
    "    using Xavier initialization, while biases are zero initialized.\n",
    "\n",
    "    Returns\n",
    "    - params: the initialized parameters\n",
    "    '''\n",
    "    def xavier_init(input_dim, output_dim, key=key):\n",
    "        '''Use Xavier initialization for weights of a single layer'''\n",
    "        std_dev = np.sqrt(2/(input_dim + output_dim)) # compute standard deviation for xavier init\n",
    "        w = std_dev * random.normal(key, (input_dim, output_dim)) # initialize the weights\n",
    "        return w\n",
    "\n",
    "    params = []\n",
    "\n",
    "    for l in range(len(layers) - 1):\n",
    "        w = xavier_init(layers[l], layers[l+1]) # xavier initialize the weight\n",
    "        b = np.zeros(layers[l+1]) # zero initialize the bias\n",
    "        params.append((w, b)) # append weight and bias for this layer to params\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def forward(params, x):\n",
    "    '''\n",
    "    Forward pass through the MLP. In PINN, the nonlinearity are\n",
    "    applied using tanh.\n",
    "\n",
    "    Arguments\n",
    "    - params: weights and biases for all layers of MLP\n",
    "    - x: input to the MLP\n",
    "\n",
    "    Returns\n",
    "    - out: output of the MLP\n",
    "    '''\n",
    "    activations = x\n",
    "    for w, b in params[:-1]:\n",
    "        out = np.dot(activations, w) + b # Perform linear operation\n",
    "        activations = np.tanh(out) # apply tanh activation\n",
    "    \n",
    "    final_w, final_b = params[-1]\n",
    "    out = np.dot(activations, final_w) + final_b # Do not apply nonlinearity to last layer\n",
    "    return out\n",
    "\n",
    "batched_forward = vmap(forward, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test implementation for `init_param` and `forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2, 10, 1]\n",
    "x = random.uniform(KEY, (5, 2))\n",
    "out = batched_forward(init_params(layers), x)\n",
    "expected = np.array([[-0.05742961],\n",
    "                     [-0.08960884],\n",
    "                     [-0.04750253],\n",
    "                     [-0.17843515],\n",
    "                     [-0.09102767]])\n",
    "assert np.allclose(out, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement `mseloss` and `update`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mseloss(params, x, true):\n",
    "    pred = batched_forward(params, x)\n",
    "    return np.mean(np.square(pred - true))\n",
    "\n",
    "def fit(params, optimizer, X, Y):\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    @jit\n",
    "    def step(params, opt_state, x, y):\n",
    "        loss_value, grads = value_and_grad(mseloss)(params, x, y)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, loss_value\n",
    "\n",
    "    for i, (x, y) in enumerate(zip(X, Y)):\n",
    "        params, opt_state, loss_value = step(params, opt_state, x, y)\n",
    "        print(f'step {i}, loss: {loss_value}')\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.10777242481708527\n",
      "step 1, loss: 0.12940318882465363\n",
      "step 2, loss: 0.1437758356332779\n",
      "step 3, loss: 0.02379879727959633\n",
      "step 4, loss: 0.36217206716537476\n",
      "step 5, loss: 0.05129792168736458\n",
      "step 6, loss: 0.00623120553791523\n",
      "step 7, loss: 0.026975566521286964\n",
      "step 8, loss: 0.006425472907721996\n",
      "step 9, loss: 0.0036361755337566137\n",
      "step 10, loss: 0.13369491696357727\n",
      "step 11, loss: 0.1718134582042694\n",
      "step 12, loss: 0.27841946482658386\n",
      "step 13, loss: 0.034345757216215134\n",
      "step 14, loss: 0.1644914746284485\n",
      "step 15, loss: 0.04430651664733887\n",
      "step 16, loss: 0.15023164451122284\n",
      "step 17, loss: 0.002893552416935563\n",
      "step 18, loss: 0.08664584904909134\n",
      "step 19, loss: 0.02422439493238926\n"
     ]
    }
   ],
   "source": [
    "layers = [5, 10, 10, 1]\n",
    "X = random.uniform(KEY, (20, 5))\n",
    "Y = random.uniform(KEY, (20, 1))\n",
    "\n",
    "optimizer = optax.adam(learning_rate=1e-2)\n",
    "trained_params = fit(init_params(layers), optimizer, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.07618722, dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mseloss(trained_params, X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('cs182_pinn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d750338c8a943fc481a1620ad51f5c8f8396d3ac2c25ef6013f469b482fa22a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
