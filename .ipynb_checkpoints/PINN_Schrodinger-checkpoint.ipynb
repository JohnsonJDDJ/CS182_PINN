{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62a4887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 8})\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import value_and_grad, jit, random, vmap, grad\n",
    "import optax\n",
    "from pyDOE import lhs\n",
    "\n",
    "from tqdm.contrib import tzip\n",
    "\n",
    "KEY = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b18a7190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = scipy.io.loadmat(\"NLS.mat\")\n",
    "\n",
    "noise = 0.0        \n",
    "    \n",
    "# Doman bounds\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "\n",
    "N0 = 50\n",
    "N_b = 50\n",
    "N_f = 20000\n",
    "layers = [2, 100, 100, 100, 100, 2]\n",
    "\n",
    "data = scipy.io.loadmat('NLS.mat')\n",
    "\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu']\n",
    "Exact_u = np.real(Exact)\n",
    "Exact_v = np.imag(Exact)\n",
    "Exact_h = np.sqrt(Exact_u**2 + Exact_v**2)\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact_u.T.flatten()[:,None]\n",
    "v_star = Exact_v.T.flatten()[:,None]\n",
    "h_star = Exact_h.T.flatten()[:,None]\n",
    "\n",
    "###########################\n",
    "\n",
    "idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "x0 = x[idx_x,:]\n",
    "u0 = Exact_u[idx_x,0:1]\n",
    "v0 = Exact_v[idx_x,0:1]\n",
    "\n",
    "idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "tb = t[idx_t,:]\n",
    "\n",
    "X_f = lb + (ub-lb)*lhs(2, N_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4ec231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN:\n",
    "    def __init__(self, x0, u0, v0, tb, X_f, layers, lb, ub):\n",
    "        X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
    "        X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "        X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
    "\n",
    "        self.lb = jnp.array(lb)\n",
    "        self.ub = jnp.array(ub)\n",
    "\n",
    "        self.x0 = jnp.array(X0[:,0:1])\n",
    "        self.t0 = jnp.array(X0[:,1:2])\n",
    "\n",
    "        self.x_lb = jnp.array(X_lb[:,0:1])\n",
    "        self.t_lb = jnp.array(X_lb[:,1:2])\n",
    "\n",
    "        self.x_ub = jnp.array(X_ub[:,0:1])\n",
    "        self.t_ub = jnp.array(X_ub[:,1:2])\n",
    "\n",
    "        self.x_f = jnp.array(X_f[:,0:1])\n",
    "        self.t_f = jnp.array(X_f[:,1:2])\n",
    "\n",
    "        self.u0 = jnp.array(u0)\n",
    "        self.v0 = jnp.array(v0)\n",
    "\n",
    "\n",
    "        # Initialize NNs\n",
    "        self.layers = layers\n",
    "        self.params = self.init_params(layers)\n",
    "        \n",
    "        \n",
    "    def init_params(self, layers, key=KEY):\n",
    "        '''\n",
    "        Initialize parameters in the MLP. Weights are initialized\n",
    "        using Xavier initialization, while biases are zero initialized.\n",
    "\n",
    "        Returns\n",
    "        - params: the initialized parameters\n",
    "        '''\n",
    "        def xavier_init(input_dim, output_dim, key=key):\n",
    "            '''Use Xavier initialization for weights of a single layer'''\n",
    "            std_dev = jnp.sqrt(2/(input_dim + output_dim)) # compute standard deviation for xavier init\n",
    "            w = std_dev * random.normal(key, (input_dim, output_dim)) # initialize the weights\n",
    "            return w\n",
    "\n",
    "        params = []\n",
    "\n",
    "        for l in range(len(layers) - 1):\n",
    "            w = xavier_init(layers[l], layers[l+1]) # xavier initialize the weight\n",
    "            b = jnp.zeros(layers[l+1]) # zero initialize the bias\n",
    "            params.append((w, b)) # append weight and bias for this layer to params\n",
    "\n",
    "        return params\n",
    "    \n",
    "    def neural_net(self, X, params):\n",
    "        num_layers = len(params) + 1\n",
    "        weights, biases = zip(*params)\n",
    "        \n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = jnp.tanh(jnp.add(jnp.matmul(H, W), b))\n",
    "#             H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = jnp.add(jnp.matmul(H, W), b)\n",
    "        return Y\n",
    "    \n",
    "    def net_u_forward(self, x, t):\n",
    "        X = jnp.concatenate([x,t],1)\n",
    "        \n",
    "        uv = self.neural_net(X, self.params)\n",
    "        u = uv[:,0:1]\n",
    "        print(u)\n",
    "        return u\n",
    "    \n",
    "    def net_v_forward(self, x, t):\n",
    "        X = jnp.concatenate([x,t],1)\n",
    "        \n",
    "        uv = self.neural_net(X, self.params)\n",
    "        v = uv[:,1:2]\n",
    "        return v\n",
    "    \n",
    "    def net_f_uv_forward(self, x, t):\n",
    "        u = self.net_u_forward(x, t)\n",
    "        v = self.net_u_forward(x, t)\n",
    "        \n",
    "        u_x = grad(u, x)[0]\n",
    "        v_x = grad(v, x)[0]\n",
    "        \n",
    "        u_t = grad(u, t)[0]\n",
    "        u_xx = grad[0]\n",
    "        \n",
    "        v_t = grad(v, t)[0]\n",
    "        v_xx = grad(v_x, x)[0]\n",
    "        \n",
    "        f_u = u_t + 0.5*v_xx + (u**2 + v**2)*v\n",
    "        f_v = v_t - 0.5*u_xx - (u**2 + v**2)*u   \n",
    "        \n",
    "        return f_u, f_v\n",
    "    \n",
    "    def loss(self, x0, t0, x_lb, t_lb, x_ub, t_ub, x_f, t_f):\n",
    "        u0_pred = self.net_u_forward(x0, t0)\n",
    "        v0_pred = self.net_v_forward(x0, t0)\n",
    "        \n",
    "        u_lb_pred = self.net_u_forward(x_lb, t_lb)\n",
    "        v_lb_pred = self.net_v_forward(x_lb, t_lb)\n",
    "        u_x_lb_pred = grad(self.net_u_forward, argnums=0)(x_lb, t_lb) # argnums=0: grad wrt 0th argument, which is x_lb\n",
    "        v_x_lb_pred = grad(self.net_v_forward, argnums=0)(x_lb, t_lb)\n",
    "        \n",
    "        u_ub_pred = self.net_u_forward(x_ub, t_ub)\n",
    "        v_ub_pred = self.net_v_forward(x_ub, t_ub)\n",
    "        u_x_ub_pred = grad(u_ub_pred, x_ub)\n",
    "        v_x_ub_pred = grad(v_ub_pred, x_ub)\n",
    "        \n",
    "        \n",
    "        f_u_pred, f_v_pred = self.net_f_uv(x_f, t_f)\n",
    "        \n",
    "        loss = jnp.mean(jnp.square(self.u0 - u0_pred)) + \\\n",
    "               jnp.mean(jnp.square(self.v0 - v0_pred)) + \\\n",
    "               jnp.mean(jnp.square(u_lb_pred - u_ub_pred)) + \\\n",
    "               jnp.mean(jnp.square(v_lb_pred - v_ub_pred)) + \\\n",
    "               jnp.mean(jnp.square(u_x_lb_pred - u_x_ub_pred)) + \\\n",
    "               jnp.mean(jnp.square(v_x_lb_pred - v_x_ub_pred)) + \\\n",
    "               jnp.mean(jnp.square(f_u_pred)) + \\\n",
    "               jnp.mean(jnp.square(f_v_pred))\n",
    "        return loss\n",
    "    \n",
    "    def train(self, nIter, optimizer):\n",
    "        opt_state = optimizer.init(self.params)\n",
    "        @jit\n",
    "        def step(params, opt_state):\n",
    "            loss_value, grads = value_and_grad(self.loss)(self.x0, self.t0, self.x_lb, self.t_lb,\n",
    "                                                          self.x_ub, self.t_ub, self.x_f, self.t_f)\n",
    "            updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "            return params, opt_state, loss_value\n",
    "        \n",
    "        train_losses = [0]\n",
    "        for it in range(nIter):\n",
    "            self.params, opt_state, loss_value = step(self.params, opt_state)\n",
    "            train_losses.append(loss_value)\n",
    "            if it % 1000 == 0:\n",
    "                print(f'Iteration {it}, average loss: {jnp.mean(loss_value)}')\n",
    "        return train_losses\n",
    "    \n",
    "    #def predict(self, Xstar):\n",
    "        \n",
    "                \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb46d776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(float32[50,1])>with<JVPTrace(level=2/1)> with\n",
      "  primal = Traced<ShapedArray(float32[50,1])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  tangent = Traced<ShapedArray(float32[50,1])>with<JaxprTrace(level=1/1)> with\n",
      "    pval = (ShapedArray(float32[50,1]), None)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7fd8b90ff580>, in_tracers=(Traced<ShapedArray(float32[50,1]):JaxprTrace(level=1/1)>,), out_tracer_refs=[<weakref at 0x7fd8b9c45890; to 'JaxprTracer' at 0x7fd8b9c45650>], out_avals=[ShapedArray(float32[50,1])], primitive=broadcast_in_dim, params={'shape': (50, 1), 'broadcast_dimensions': (0, 1)}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7fd8b9c0f7b0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
      "Traced<ShapedArray(float32[50,1])>with<DynamicJaxprTrace(level=0/1)>\n",
      "Traced<ShapedArray(float32[50,1])>with<JVPTrace(level=4/1)> with\n",
      "  primal = Traced<ShapedArray(float32[50,1])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  tangent = Traced<ShapedArray(float32[50,1])>with<JaxprTrace(level=3/1)> with\n",
      "    pval = (ShapedArray(float32[50,1]), None)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7fd8b90ff550>, in_tracers=(Traced<ShapedArray(float32[50,1]):JaxprTrace(level=3/1)>,), out_tracer_refs=[<weakref at 0x7fd8b9d5b290; to 'JaxprTracer' at 0x7fd8b9d5b530>], out_avals=[ShapedArray(float32[50,1])], primitive=broadcast_in_dim, params={'shape': (50, 1), 'broadcast_dimensions': (0, 1)}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7fd8b9ca5d30>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Gradient only defined for scalar-output functions. Output had shape: (50, 1).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sy/z6c69pg15pv4t5hlfmll0x7w0000gn/T/ipykernel_7520/2993912813.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training time: %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/sy/z6c69pg15pv4t5hlfmll0x7w0000gn/T/ipykernel_7520/2841874567.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, nIter, optimizer)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "\u001b[0;32m/var/folders/sy/z6c69pg15pv4t5hlfmll0x7w0000gn/T/ipykernel_7520/2841874567.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(params, opt_state)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             loss_value, grads = value_and_grad(self.loss)(self.x0, self.t0, self.x_lb, self.t_lb,\n\u001b[0;32m--> 134\u001b[0;31m                                                           self.x_ub, self.t_ub, self.x_f, self.t_f)\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "\u001b[0;32m/var/folders/sy/z6c69pg15pv4t5hlfmll0x7w0000gn/T/ipykernel_7520/2841874567.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x0, t0, x_lb, t_lb, x_ub, t_ub, x_f, t_f)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mu_lb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_u_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_lb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_lb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mv_lb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_v_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_lb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_lb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mu_x_lb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_u_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_lb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_lb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# argnums=0: grad wrt 0th argument, which is x_lb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mv_x_lb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_v_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_lb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_lb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pinnenv/lib/python3.7/site-packages/jax/_src/api.py\u001b[0m in \u001b[0;36m_check_scalar\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mShapedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"had shape: {aval.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"had abstract value {aval}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Gradient only defined for scalar-output functions. Output had shape: (50, 1)."
     ]
    }
   ],
   "source": [
    "# Train and Test\n",
    "model = PINN(x0, u0, v0, tb, X_f, layers, lb, ub)\n",
    "optimizer = optax.adam(learning_rate=5e-4)\n",
    "             \n",
    "start_time = time.time()                \n",
    "model.train(1000, optimizer)\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.4f' % (elapsed))\n",
    "\n",
    "\n",
    "# u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star)\n",
    "# h_pred = np.sqrt(u_pred**2 + v_pred**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c40f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
